# üçá AVD - Pipeline de BI Clim√°tico para Viticultura 

## 1. Introdu√ß√£o e Objetivo

Este projeto implementa um pipeline de Business Intelligence (BI) para an√°lise e visualiza√ß√£o de dados meteorol√≥gicos do INMET (Instituto Nacional de Meteorologia), focando no estado de Pernambuco, com √™nfase no **Vale do S√£o Francisco**.

O objetivo central √© aplicar t√©cnicas de **Agrupamento (Clustering) K-Means** para identificar **Padr√µes Clim√°ticos Chave** durante fases cr√≠ticas da videira, como a flora√ß√£o e a matura√ß√£o, utilizando dados agregados de temperatura, umidade e radia√ß√£o solar. O resultado deste agrupamento deve ser visualizado em dashboards interativos (ThingsBoard/Trendz).

## 2. Membros do Projeto

| Nome | Usu√°rio |
| :--- | :--- |
| Lisa Matubara | `lm` |
| Luziane Santos | `lps` |
| Maria J√∫lia Peixoto | `mjpo` |
| Matheus Velame | `mvp2` |
| Paulo Rago | `prcr` |
| Tha√≠s Aguiar | `thcba` |

* **Disciplina:** An√°lise e Visualiza√ß√£o de Dados - 2025.2
* **Institui√ß√£o:** CESAR School

## 3. Arquitetura do Pipeline

A solu√ß√£o √© baseada em cont√™ineres Docker e orquestrada via Docker Compose, abrangendo as seguintes camadas:

| Servi√ßo | Fun√ß√£o Principal | Porta |
| :--- | :--- | :--- |
| **FastAPI** | Interface de ingest√£o dos dados brutos do INMET e integra√ß√£o com MinIO/S3. | `8060` |
| **MinIO/S3** | Armazenamento de dados brutos e modelos. | - |
| **Snowflake** | Estrutura√ß√£o de dados tratados (Simulado por `SQLite`/`PostgreSQL` em ambiente local). | - |
| **Jupyter Notebook** | Ambiente de limpeza, agrega√ß√£o de features e modelagem K-Means. | `8888` |
| **MLFlow** | Registro e versionamento do modelo de K-Means e artefatos. | `5000` |
| **Trendz Analytics** | Visualiza√ß√£o dos dados e dashboards interativos. | `8888` |

**Fluxo Geral:**

1. Os dados brutos do INMET s√£o ingeridos e salvos no S3/MinIO.
2. Os dados s√£o estruturados no Snowflake .
3. O Jupyter Notebook l√™ a base estruturada, aplica o K-Means e registra o modelo no MLFlow.
4. O dashboard no ThingsBoard/Trendz consome os resultados do agrupamento para gerar visualiza√ß√µes de padr√µes clim√°ticos.

## 4. Estrutura do Reposit√≥rio

| Caminho | Descri√ß√£o |
| :--- | :--- |
| `docker-compose.yml` | Orquestra√ß√£o dos cont√™ineres da infraestrutura. |
| `fastapi/` | Camada de ingest√£o de dados (API). |
| `jupyterlab/` | Dockerfile e configs do ambiente Jupyter. |
| `mlflow/` | Configura√ß√£o e armazenamento de experimentos. |
| `notebooks/` | Notebooks de tratamento, modelagem e visualiza√ß√£o. |
| `sql_scripts/` | Scripts SQL de estrutura√ß√£o e consultas (DML/DDL). |
| `reports/` | **Local de entrega do Relat√≥rio T√©cnico em PDF**. |
| `trendz/` | Dashboards e configura√ß√µes exportadas. |

## 5. Instru√ß√µes de Execu√ß√£o

Siga os passos abaixo para levantar a infraestrutura, executar o pipeline e visualizar o dashboard:

### 5.1. Pr√©-requisitos

* Docker e Docker Compose instalados.
* Conex√£o est√°vel com a internet.

### 5.2. Subir a Infraestrutura

1.  [Clone este reposit√≥rio](https://docs.github.com/pt/repositories/creating-and-managing-repositories/creating-a-new-repository) e entre na raiz do projeto:
    ```bash
    cd avd-projeto
    ```
2.  Construa as imagens e suba todos os servi√ßos definidos no `docker-compose.yml`:
    ```bash
    docker-compose up -d --build
    ```

### 5.3. Execu√ß√£o do Pipeline

1.  Acesse o Jupyter Notebook (porta `8888`): `http://localhost:8888`
2.  Execute o notebook **`01_tratamento_dados_inmet.ipynb`** para:
    * Carregar dados brutos (do `/data/raw`).
    * Limpar nulos (interpola√ß√£o) e salvar dados tratados (no `/data/processed`).
3.  **[ETAPA MANUAL: Carregamento para o Banco de Dados]**
    * Execute os scripts SQL em `sql_scripts/` para criar o schema no Snowflake (ou Postres/SQLite).
    * Use o FastAPI (`main.py`) ou um script auxiliar no Jupyter para carregar os dados tratados (CSV em `/data/processed`) para a tabela do Snowflake.
4.  Execute o notebook **`02_modelagem_kmeans.ipynb`** para:
    * Ler os dados estruturados do Snowflake.
    * Tratar Outliers e Aggregar features (Semanal).
    * Treinar e registrar o modelo K-Means no MLFlow (`http://localhost:5000`).

### 5.4. Visualiza√ß√£o do Dashboard

1.  Acesse o Trendz Analytics (porta `8888` - pode ser a mesma do Jupyter se o `docker-compose.yml` for diferente): `http://localhost:8888`
2.  Importe o dashboard de agrupamento (arquivos em `trendz/`).
3.  O dashboard deve exibir:
    * A distribui√ß√£o das semanas nos clusters identificados.
    * Gr√°ficos de dispers√£o coloridos por cluster para vari√°veis-chave (e.g., Temperatura vs. Umidade).
    * Pain√©is com as m√©dias de cada grupo clim√°tico.

## 6. Resultados e Conclus√µes

* **Relat√≥rio T√©cnico:** O relat√≥rio final em PDF, contendo a arquitetura, metodologia, resultados e conclus√µes, ser√° salvo no diret√≥rio `/reports/` antes da entrega.