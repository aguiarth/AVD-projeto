# üå¶Ô∏è AVD - Pipeline de BI Clim√°tico

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11-blue.svg)
![Docker](https://img.shields.io/badge/Docker-20.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)
![License](https://img.shields.io/badge/License-Academic-lightgrey.svg)

**Pipeline completo de Business Intelligence para an√°lise e visualiza√ß√£o de dados meteorol√≥gicos do INMET**

[üìã √çndice](#-√≠ndice) ‚Ä¢ [üöÄ In√≠cio R√°pido](#-in√≠cio-r√°pido) ‚Ä¢ [üìö Documenta√ß√£o](#-documenta√ß√£o) ‚Ä¢ [ü§ù Equipe](#-equipe)

</div>

---

## üìã √çndice

1. [Introdu√ß√£o e Objetivo](#1-introdu√ß√£o-e-objetivo)
2. [Membros do Projeto](#2-membros-do-projeto)
3. [Arquitetura do Pipeline](#3-arquitetura-do-pipeline)
4. [Estrutura do Reposit√≥rio](#4-estrutura-do-reposit√≥rio)
5. [Tecnologias Utilizadas](#5-tecnologias-utilizadas)
6. [Requisitos e Depend√™ncias](#6-requisitos-e-depend√™ncias)
7. [Instala√ß√£o e Configura√ß√£o](#7-instala√ß√£o-e-configura√ß√£o)
8. [Instru√ß√µes de Execu√ß√£o](#8-instru√ß√µes-de-execu√ß√£o)
9. [Notebooks do Projeto](#9-notebooks-do-projeto)
10. [Scripts Auxiliares](#10-scripts-auxiliares)
11. [Troubleshooting](#11-troubleshooting)
12. [Resultados e Conclus√µes](#12-resultados-e-conclus√µes)

---

## 1. Introdu√ß√£o e Objetivo

Este projeto implementa um **pipeline completo de Business Intelligence (BI)** para an√°lise e visualiza√ß√£o de dados meteorol√≥gicos do **INMET (Instituto Nacional de Meteorologia)**, focando no estado de **Pernambuco**, com √™nfase nas esta√ß√µes de **Petrolina** e **Garanhuns**.

### üéØ Objetivo Central

Aplicar t√©cnicas de **Agrupamento (Clustering) K-Means** para identificar **Padr√µes Clim√°ticos Chave** utilizando dados agregados de temperatura, umidade, radia√ß√£o solar, precipita√ß√£o e press√£o atmosf√©rica. O resultado deste agrupamento √© visualizado em dashboards interativos no **ThingsBoard**.

### üìä Dados Processados

- **Per√≠odo:** 2020 a 2024
- **Esta√ß√µes:** Petrolina (A307) e Garanhuns (A322)
- **Frequ√™ncia:** Dados hor√°rios
- **Vari√°veis:** Temperatura, Umidade, Radia√ß√£o, Vento, Precipita√ß√£o, Press√£o

---

## 2. Membros do Projeto

| Nome | Usu√°rio |
| :--- | :--- |
| Lisa Matubara | `lilymtbr` |
| Luziane Santos | `luzianes` |
| Maria J√∫lia Peixoto | `majupeixoto` |
| Matheus Velame | `MatheusVelame` |
| Paulo Rago | `paulo_rago` |
| Tha√≠s Aguiar | `aguiarth` |

* **Disciplina:** An√°lise e Visualiza√ß√£o de Dados - 2025.2  
* **Institui√ß√£o:** CESAR School

---

## 3. Arquitetura do Pipeline

A solu√ß√£o √© baseada em cont√™ineres **Docker** e orquestrada via **Docker Compose**, abrangendo as seguintes camadas:

| Servi√ßo | Fun√ß√£o Principal | Porta | URL de Acesso |
| :--- | :--- | :--- | :--- |
| **JupyterLab** | Ambiente de an√°lise, tratamento de dados e modelagem | `8888` | `http://localhost:8888` |
| **FastAPI** | Interface de ingest√£o dos dados brutos do INMET e integra√ß√£o com MinIO/S3 | `8000` | `http://localhost:8000` |
| **MinIO/S3** | Data Lake - Armazenamento de dados brutos e modelos | `9000` (API)<br>`9001` (Console) | `http://localhost:9001` |
| **PostgreSQL** | Data Warehouse - Banco de dados relacional para armazenamento estruturado | `5432` | `http://localhost:8085` (Adminer) |
| **Adminer** | Interface web para gerenciamento do PostgreSQL | `8085` | `http://localhost:8085` |
| **MLFlow** | Registro e versionamento do modelo de K-Means e artefatos | `5000` | `http://localhost:5000` |
| **ThingsBoard** | Plataforma IoT para visualiza√ß√£o de dados e dashboards | `8090` | `http://localhost:8090` |

### üîÑ Fluxo Detalhado do Pipeline

```mermaid
graph TD
    A[Dados Limpos] --> B[send_inmet_to_tb.py]
    B --> C[ThingsBoard]
    C -->|Regra de Neg√≥cio| D[MinIO/S3]
    D -->|ETL| E[PostgreSQL]
    E -->|Extra√ß√£o| F[Jupyter Notebook]
    F -->|K-Means| G[MLFlow]
    F --> H[Modelos e Resultados]
    H --> C
```

#### Fluxo de Dados Detalhado

1. **Ingest√£o:** Os dados limpos s√£o enviados ao ThingsBoard via script Python (`scripts/send_inmet_to_tb.py`).

2. **ThingsBoard ‚Üí MinIO:** O ThingsBoard aplica uma **Regra de Neg√≥cio** para persistir os dados brutos no MinIO/S3 (Data Lake).

3. **MinIO ‚Üí PostgreSQL:** A transfer√™ncia do data lake (MinIO) para o data warehouse (PostgreSQL) √© realizada atrav√©s de um script de ETL dedicado (`scripts/etl_minio_to_postgres.py`).

4. **Modelagem:** O Jupyter Notebook extrai os dados estruturados diretamente do PostgreSQL para o Machine Learning (K-Means), que √© rastreado pelo MLFlow.

5. **Visualiza√ß√£o:** O ThingsBoard consome os resultados do agrupamento para gerar dashboards interativos.

---

## 4. Estrutura do Reposit√≥rio

```
AVD-projeto-1/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Dados brutos do INMET (CSV)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2020/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2021/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2022/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2023/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2024/
‚îÇ   ‚îî‚îÄ‚îÄ processed/               # Dados tratados (CSV)
‚îÇ       ‚îú‚îÄ‚îÄ petrolina_*_tratado.csv
‚îÇ       ‚îú‚îÄ‚îÄ garanhuns_*_tratado.csv
‚îÇ       ‚îú‚îÄ‚îÄ dados_semanais_clustered.csv
‚îÇ       ‚îú‚îÄ‚îÄ modelos_backup/      # Modelos ML salvos
‚îÇ       ‚îú‚îÄ‚îÄ modelos_viticultura/ # Modelos espec√≠ficos
‚îÇ       ‚îî‚îÄ‚îÄ temp_models/         # Modelos tempor√°rios
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_carregar_dados.ipynb          # Notebook explorat√≥rio
‚îÇ   ‚îú‚îÄ‚îÄ 01_tratamento_dados_inmet.ipynb  # Processamento completo
‚îÇ   ‚îî‚îÄ‚îÄ 02_Modelagem.ipynb         # Modelagem e clustering
‚îú‚îÄ‚îÄ fastapi/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # API de ingest√£o
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt         # Depend√™ncias FastAPI
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile               # Imagem Docker FastAPI
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ etl_minio_to_postgres.py    # ETL MinIO ‚Üí PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ send_inmet_to_tb.py          # Envio de dados para ThingsBoard
‚îÇ   ‚îú‚îÄ‚îÄ test_pipeline.py             # Testes do pipeline
‚îú‚îÄ‚îÄ mlflow/
‚îÇ   ‚îî‚îÄ‚îÄ artifacts/                # Artefatos dos modelos
‚îú‚îÄ‚îÄ minio/
‚îÇ   ‚îî‚îÄ‚îÄ data/                     # Dados armazenados no MinIO
‚îú‚îÄ‚îÄ thingsboard/
‚îÇ   ‚îú‚îÄ‚îÄ data/                     # Banco de dados do ThingsBoard
‚îÇ   ‚îî‚îÄ‚îÄ logs/                     # Logs do ThingsBoard
‚îú‚îÄ‚îÄ docker-compose.yml            # Orquestra√ß√£o dos servi√ßos
‚îú‚îÄ‚îÄ Dockerfile.jupyter            # Dockerfile do Jupyter
‚îî‚îÄ‚îÄ README.md                     # Este arquivo
```

---

## 5. Tecnologias Utilizadas

### üê≥ Infraestrutura e Containeriza√ß√£o
- **Docker** (20.10+) - Containeriza√ß√£o de aplica√ß√µes
- **Docker Compose** (2.0+) - Orquestra√ß√£o de servi√ßos

### üîß Backend e APIs
- **FastAPI** (0.100+) - Framework web moderno para APIs REST
- **Uvicorn** - Servidor ASGI de alta performance
- **Python 3.11** - Linguagem de programa√ß√£o principal

### üíæ Armazenamento de Dados
- **MinIO** - Data Lake - Armazenamento de objetos compat√≠vel com S3
- **PostgreSQL 15** - Data Warehouse - Banco de dados relacional
- **SQLAlchemy** - ORM para Python

### üìä An√°lise de Dados e Machine Learning
- **Pandas** - Manipula√ß√£o e an√°lise de dados
- **NumPy** - Computa√ß√£o num√©rica
- **Scikit-learn** - Machine Learning (K-Means, StandardScaler)
- **JupyterLab** - Ambiente de desenvolvimento interativo

### üìà Visualiza√ß√£o
- **Matplotlib** - Visualiza√ß√µes est√°ticas
- **Seaborn** - Visualiza√ß√µes estat√≠sticas avan√ßadas

### üîÑ MLOps e Versionamento
- **MLFlow** (v2.7.1) - Gerenciamento do ciclo de vida de modelos
- **Git** - Controle de vers√£o

### üåê IoT e Visualiza√ß√£o
- **ThingsBoard** - Plataforma IoT para visualiza√ß√£o e dashboards
- **Adminer** - Interface web para PostgreSQL

### üì° Integra√ß√£o e Comunica√ß√£o
- **Requests** - Cliente HTTP para Python
- **psycopg2-binary** - Adaptador PostgreSQL para Python
- **python-multipart** - Suporte para upload de arquivos

---

## 6. Requisitos e Depend√™ncias

### üì¶ Depend√™ncias do FastAPI

Arquivo: `fastapi/requirements.txt`

```
fastapi
uvicorn[standard]
pandas
python-multipart
minio
requests
psycopg2-binary
sqlalchemy
```

### üì¶ Depend√™ncias do JupyterLab

Instaladas via `Dockerfile.jupyter`:

```
minio
psycopg2-binary
sqlalchemy
```

### üì¶ Depend√™ncias dos Notebooks

Bibliotecas Python utilizadas nos notebooks:

```python
# An√°lise de Dados
pandas
numpy

# Machine Learning
scikit-learn

# Visualiza√ß√£o
matplotlib
seaborn

# Integra√ß√£o
minio
psycopg2-binary
sqlalchemy
requests
```
```

### üì¶ Depend√™ncias dos Scripts

```python
# ETL e Processamento
pandas
minio
sqlalchemy
psycopg2-binary
```

# Comunica√ß√£o
requests
```

---

## 7. Instala√ß√£o e Configura√ß√£o

### 7.1. Pr√©-requisitos

#### Linux / macOS

```bash
# Verificar vers√£o do Docker
docker --version

# Verificar vers√£o do Docker Compose
docker-compose --version

# Instalar Docker (se necess√°rio)
# Ubuntu/Debian:
sudo apt-get update
sudo apt-get install docker.io docker-compose

# macOS (via Homebrew):
brew install docker docker-compose
```

#### Windows

```powershell
# Verificar vers√£o do Docker
docker --version

# Verificar vers√£o do Docker Compose
docker-compose --version

# Instalar Docker Desktop (inclui Docker Compose)
# Baixar de: https://www.docker.com/products/docker-desktop
```

### 7.2. Requisitos do Sistema

- **Docker** (vers√£o 20.10 ou superior)
- **Docker Compose** (vers√£o 2.0 ou superior)
- **Git** (para clonar o reposit√≥rio)
- **8GB de RAM** (recomendado)
- **10GB de espa√ßo em disco** (para dados e imagens)
- **Conex√£o est√°vel com a internet** (para download de imagens Docker)

### 7.3. Clonagem do Reposit√≥rio

#### Linux / macOS

```bash
git clone <git@github.com:aguiarth/AVD-projeto.git>
cd AVD-projeto
```

#### Windows (PowerShell)

```powershell
git clone <git@github.com:aguiarth/AVD-projeto.git>
cd AVD-projeto
```

#### Windows (CMD)

```cmd
git clone <git@github.com:aguiarth/AVD-projeto.git>
cd AVD-projeto
```

---

## 8. Instru√ß√µes de Execu√ß√£o

### 8.1. Subir a Infraestrutura

#### Linux / macOS

```bash
# Construir as imagens e iniciar os servi√ßos
docker-compose up -d --build

# Verificar se todos os servi√ßos est√£o rodando
docker-compose ps
```

#### Windows (PowerShell)

```powershell
# Construir as imagens e iniciar os servi√ßos
docker-compose up -d --build

# Verificar se todos os servi√ßos est√£o rodando
docker-compose ps
```

#### Windows (CMD)

```cmd
docker-compose up -d --build
docker-compose ps
```

### 8.2. Verificar Status dos Servi√ßos

Voc√™ deve ver todos os servi√ßos com status `Up`:

- `jupyter-uva` (JupyterLab)
- `thingsboard` (ThingsBoard)
- `fastapi-clima` (FastAPI)
- `minio` (MinIO)
- `mlflow_server` (MLFlow)
- `postgres-avd` (PostgreSQL)
- `adminer-avd` (Adminer)

### 8.3. Acessar os Servi√ßos

| Servi√ßo | URL | Credenciais |
| :--- | :--- | :--- |
| **JupyterLab** | `http://localhost:8888` | Sem token |
| **FastAPI** | `http://localhost:8000` | - |
| **FastAPI Docs** | `http://localhost:8000/docs` | - |
| **MinIO Console** | `http://localhost:9001` | `admin` / `admin12345` |
| **MLFlow** | `http://localhost:5000` | - |
| **ThingsBoard** | `http://localhost:8090` | `tenant@thingsboard.org` / `tenant` |
| **Adminer** | `http://localhost:8085` | Sistema: `PostgreSQL`<br>Servidor: `postgres`<br>Usu√°rio: `postgres`<br>Senha: `postgres`<br>Base de dados: `clima` |

### 8.4. Execu√ß√£o do Pipeline

#### Passo 1: Processamento dos Dados

1. **Acesse o JupyterLab:** `http://localhost:8888`

2. **Execute o notebook `01_tratamento_dados_inmet.ipynb`:**
   - Este notebook processa todos os arquivos CSV do INMET (2020-2024)
   - Aplica limpeza, interpola√ß√£o temporal e tratamento de valores faltantes
   - Salva os dados tratados em `/data/processed/`

   **Vari√°veis processadas:**
   - Temperatura do ar (¬∞C)
   - Umidade relativa (%)
   - Radia√ß√£o global (kJ/m¬≤) - quando dispon√≠vel
   - Velocidade do vento (m/s)
   - Precipita√ß√£o (mm)
   - Press√£o atmosf√©rica (mB)

3. **Para explora√ß√£o r√°pida, use o notebook `01_carregar_dados.ipynb`:**
   - Permite visualizar e explorar um arquivo espec√≠fico
   - Usa a mesma fun√ß√£o de processamento do notebook principal

#### Passo 2: Modelagem K-Means

1. **Execute o notebook `02_Modelagem_KMeans.ipynb`:**
   - Carrega os dados estruturados diretamente do PostgreSQL
   - Agrega dados por semana
   - Trata outliers
   - Aplica normaliza√ß√£o (StandardScaler)
   - Treina o modelo K-Means
   - Avalia o modelo (silhouette score)
   - Registra o modelo no MLFlow

   **Vari√°veis utilizadas na agrega√ß√£o semanal:**
   - Temperatura (m√©dia e desvio padr√£o)
   - Umidade (m√©dia e m√≠nima)
   - Radia√ß√£o (soma)
   - Precipita√ß√£o (soma)
   - Press√£o (m√©dia)
   
   *Nota: A velocidade do vento √© processada nos dados brutos, mas n√£o √© utilizada na agrega√ß√£o semanal para o modelo K-Means.*

2. **Visualizar o modelo no MLFlow:**
   - Acesse `http://localhost:5000`
   - Navegue at√© o experimento "K-Means Clustering"
   - Visualize m√©tricas, par√¢metros e artefatos

#### Passo 3: Visualiza√ß√£o no ThingsBoard

1. **Acesse o ThingsBoard:** `http://localhost:8090`
   - Credenciais padr√£o: `tenant@thingsboard.org` / `tenant`

2. **Configure dispositivos e dashboards:**
   - Crie dispositivos para cada esta√ß√£o (Petrolina, Garanhuns)
   - Configure uma **Regra de Neg√≥cio** no ThingsBoard para persistir dados no MinIO
   - Use o script `scripts/send_inmet_to_tb.py` para enviar dados limpos ao ThingsBoard
   - Execute o script `scripts/etl_minio_to_postgres.py` para transferir dados do MinIO para PostgreSQL
   - Crie dashboards para visualizar os clusters identificados

### 8.5. Executar Scripts Auxiliares

#### Linux / macOS

```bash
# Enviar dados limpos para ThingsBoard
python scripts/send_inmet_to_tb.py

# ETL MinIO ‚Üí PostgreSQL (ap√≥s ThingsBoard persistir no MinIO)
python scripts/etl_minio_to_postgres.py

# Testar pipeline
python scripts/test_pipeline.py
```

#### Windows (PowerShell)

```powershell
# Enviar dados limpos para ThingsBoard
python scripts/send_inmet_to_tb.py

# ETL MinIO ‚Üí PostgreSQL (ap√≥s ThingsBoard persistir no MinIO)
python scripts/etl_minio_to_postgres.py

# Testar pipeline
python scripts/test_pipeline.py
```

#### Windows (CMD)

```cmd
python scripts\send_inmet_to_tb.py
python scripts\etl_minio_to_postgres.py
python scripts\test_pipeline.py
```

---

## 9. Notebooks do Projeto

### üìì `01_carregar_dados.ipynb`

**Prop√≥sito:** Notebook explorat√≥rio para visualiza√ß√£o e an√°lise r√°pida de dados.

**Funcionalidades:**
- Lista arquivos CSV dispon√≠veis
- Processa um arquivo espec√≠fico usando a fun√ß√£o `processar_inmet()`
- Exibe estat√≠sticas descritivas
- Verifica valores faltantes
- Visualiza amostras dos dados

**Quando usar:** Para explora√ß√£o inicial dos dados ou an√°lise de um arquivo espec√≠fico.

### üìì `01_tratamento_dados_inmet.ipynb`

**Prop√≥sito:** Processamento completo de todos os arquivos do INMET.

**Funcionalidades:**
- Processa todos os arquivos CSV (2020-2024, Petrolina e Garanhuns)
- Aplica fun√ß√£o `processar_inmet()` padronizada
- Interpola valores faltantes usando m√©todo temporal
- Remove colunas 100% vazias (ex: radia√ß√£o quando ausente)
- Cria features auxiliares (hora_num, mes)
- Salva dados tratados em CSV em `/data/processed/`

**Tratamento aplicado:**
- Convers√£o de v√≠rgula para ponto decimal
- Padroniza√ß√£o de formato de hora
- Cria√ß√£o de √≠ndice datetime
- Interpola√ß√£o temporal de valores faltantes
- Preenchimento de bordas (ffill/bfill)

**Quando usar:** Para processar todos os dados e preparar para modelagem.

### üìì `02_Modelagem_KMeans.ipynb`

**Prop√≥sito:** Modelagem de clustering para identificar padr√µes clim√°ticos.

**Funcionalidades:**
- Extrai dados estruturados diretamente do PostgreSQL
- Agrega√ß√£o semanal dos dados hor√°rios
- Tratamento de outliers
- Normaliza√ß√£o com StandardScaler
- Treinamento de K-Means
- Avalia√ß√£o com silhouette score
- Visualiza√ß√£o dos clusters
- Registro no MLFlow

**Vari√°veis utilizadas na agrega√ß√£o semanal:**
- Temperatura (m√©dia e desvio padr√£o)
- Umidade (m√©dia e m√≠nima)
- Radia√ß√£o (soma)
- Precipita√ß√£o (soma)
- Press√£o (m√©dia)

*Nota: A velocidade do vento √© processada nos dados brutos, mas n√£o √© utilizada na agrega√ß√£o semanal para o modelo K-Means.*

**Quando usar:** Ap√≥s o processamento dos dados, para identificar padr√µes clim√°ticos.

---

## 10. Scripts Auxiliares

### üîß `scripts/etl_minio_to_postgres.py`

Script de ETL para transferir dados do Data Lake (MinIO) para o Data Warehouse (PostgreSQL). Este script deve ser executado ap√≥s o ThingsBoard persistir os dados no MinIO atrav√©s de sua Regra de Neg√≥cio.

**Uso:**

```bash
# Linux / macOS
python scripts/etl_minio_to_postgres.py

# Windows
python scripts\etl_minio_to_postgres.py
```

**Funcionalidades:**
- Conecta ao MinIO e lista arquivos CSV
- Carrega dados do MinIO (dados brutos persistidos pelo ThingsBoard)
- Cria tabela `inmet_raw` no PostgreSQL (se n√£o existir)
- Insere dados na tabela `inmet_raw` do PostgreSQL
- Organiza dados por dispositivo (Petrolina/Garanhuns)

**Fluxo:**
1. Execute `send_inmet_to_tb.py` para enviar dados ao ThingsBoard
2. O ThingsBoard persiste dados brutos no MinIO via Regra de Neg√≥cio
3. Execute este script para transferir dados do MinIO para PostgreSQL
4. O Jupyter Notebook extrai dados do PostgreSQL para modelagem

### üîß `scripts/send_inmet_to_tb.py`

Script para enviar dados limpos processados para o ThingsBoard. Este √© o primeiro passo do pipeline de dados.

**Uso:**

```bash
# Linux / macOS
python scripts/send_inmet_to_tb.py

# Windows
python scripts\send_inmet_to_tb.py
```

**Funcionalidades:**
- L√™ CSVs tratados de `data/processed/`
- Envia telemetria linha por linha para o ThingsBoard
- Suporta m√∫ltiplos dispositivos (Petrolina, Garanhuns)
- Inclui delay para n√£o sobrecarregar o ThingsBoard

**Configura√ß√£o necess√°ria:**
- Editar tokens dos dispositivos no dicion√°rio `DEVICES`

**Fluxo:**
1. Este script envia dados limpos para o ThingsBoard
2. O ThingsBoard aplica uma Regra de Neg√≥cio para persistir dados brutos no MinIO
3. Execute `etl_minio_to_postgres.py` para transferir dados do MinIO para PostgreSQL

### üîß `scripts/test_pipeline.py`

Script de testes para validar o pipeline completo.

**Uso:**

```bash
# Linux / macOS
python scripts/test_pipeline.py

# Windows
python scripts\test_pipeline.py
```

---

## 11. Troubleshooting

### ‚ùå Problema: Servi√ßos n√£o iniciam

**Solu√ß√£o:**

#### Linux / macOS

```bash
# Verificar logs
docker-compose logs

# Reiniciar servi√ßos
docker-compose restart

# Reconstruir imagens
docker-compose up -d --build --force-recreate
```

#### Windows (PowerShell)

```powershell
docker-compose logs
docker-compose restart
docker-compose up -d --build --force-recreate
```

#### Windows (CMD)

```cmd
docker-compose logs
docker-compose restart
docker-compose up -d --build --force-recreate
```

### ‚ùå Problema: Porta j√° em uso

**Solu√ß√£o:**
- Verifique se outra aplica√ß√£o est√° usando a porta
- Altere a porta no `docker-compose.yml` se necess√°rio
- Use `docker-compose down` antes de subir novamente

#### Linux / macOS

```bash
# Verificar processos usando a porta
sudo lsof -i :8888  # Para porta 8888
sudo netstat -tulpn | grep :8888

# Parar servi√ßos
docker-compose down
```

#### Windows (PowerShell)

```powershell
# Verificar processos usando a porta
netstat -ano | findstr :8888

# Parar servi√ßos
docker-compose down
```

### ‚ùå Problema: Erro ao processar dados

**Solu√ß√£o:**
- Verifique se os arquivos CSV est√£o em `/data/raw/` com a estrutura correta
- Confirme que o encoding √© `latin1`
- Verifique os logs do Jupyter: `docker-compose logs jupyterlab`

#### Linux / macOS

```bash
docker-compose logs jupyterlab
```

#### Windows

```powershell
docker-compose logs jupyterlab
```

### ‚ùå Problema: PostgreSQL n√£o conecta

**Solu√ß√£o:**
- Verifique se o servi√ßo est√° rodando: `docker-compose ps postgres`
- Confirme credenciais: `postgres` / `postgres`
- Acesse via Adminer: `http://localhost:8085`
- Verifique a conex√£o no notebook: `postgresql://postgres:postgres@postgres:5432/clima`

### ‚ùå Problema: MLFlow n√£o salva modelos

**Solu√ß√£o:**
- Verifique se o volume `./mlflow` est√° montado corretamente
- Confirme permiss√µes de escrita no diret√≥rio
- Verifique logs: `docker-compose logs mlflow`

#### Linux / macOS

```bash
# Verificar permiss√µes
ls -la mlflow/

# Ver logs
docker-compose logs mlflow
```

#### Windows

```powershell
docker-compose logs mlflow
```

### ‚ùå Problema: PostgreSQL n√£o conecta

**Solu√ß√£o:**
- Verifique se o servi√ßo est√° rodando: `docker-compose ps postgres`
- Confirme credenciais: `postgres` / `postgres`
- Acesse via Adminer: `http://localhost:8085`

### üõ†Ô∏è Comandos √öteis

#### Linux / macOS

```bash
# Parar todos os servi√ßos
docker-compose down

# Parar e remover volumes
docker-compose down -v

# Ver logs de um servi√ßo espec√≠fico
docker-compose logs -f jupyterlab

# Executar comando em um container
docker-compose exec jupyterlab bash

# Limpar recursos n√£o utilizados
docker system prune -a

# Ver uso de recursos
docker stats
```

#### Windows (PowerShell)

```powershell
# Parar todos os servi√ßos
docker-compose down

# Parar e remover volumes
docker-compose down -v

# Ver logs de um servi√ßo espec√≠fico
docker-compose logs -f jupyterlab

# Executar comando em um container
docker-compose exec jupyterlab bash

# Limpar recursos n√£o utilizados
docker system prune -a

# Ver uso de recursos
docker stats
```

#### Windows (CMD)

```cmd
docker-compose down
docker-compose down -v
docker-compose logs -f jupyterlab
docker-compose exec jupyterlab bash
docker system prune -a
docker stats
```

---

## 12. Resultados e Conclus√µes

### üìä Dados Processados

- **Total de registros:** ~87.000 registros hor√°rios (por ano)
- **Per√≠odo:** 2020-2024
- **Esta√ß√µes:** 2 (Petrolina e Garanhuns)
- **Vari√°veis clim√°ticas:** 6 principais

### ü§ñ Modelo K-Means

- **M√©todo:** Clustering n√£o-supervisionado
- **Features:** Agrega√ß√µes semanais de vari√°veis clim√°ticas
- **Avalia√ß√£o:** Silhouette score
- **Versionamento:** MLFlow

### üìà Visualiza√ß√£o

- **Plataforma:** ThingsBoard
- **Dashboards:** Padr√µes clim√°ticos por cluster
- **Interatividade:** Filtros por per√≠odo, esta√ß√£o e vari√°vel

### üìÑ Relat√≥rio T√©cnico

O relat√≥rio final em PDF, contendo a arquitetura, metodologia, resultados e conclus√µes, ser√° enviado junto da entrega.

---

## üìö Refer√™ncias

- [INMET - Instituto Nacional de Meteorologia](https://portal.inmet.gov.br/)
- [ThingsBoard - Documenta√ß√£o](https://thingsboard.io/docs/)
- [MLFlow - Documenta√ß√£o](https://www.mlflow.org/docs/latest/index.html)
- [Scikit-learn K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)
- [FastAPI - Documenta√ß√£o](https://fastapi.tiangolo.com/)
- [Docker - Documenta√ß√£o](https://docs.docker.com/)
- [PostgreSQL - Documenta√ß√£o](https://www.postgresql.org/docs/)
- [MinIO - Documenta√ß√£o](https://min.io/docs/)

---

## üìù Licen√ßa

Este projeto √© desenvolvido para fins acad√™micos no contexto da disciplina de An√°lise e Visualiza√ß√£o de Dados da CESAR School.

---

## ü§ù Equipe

Este √© um projeto acad√™mico desenvolvido pela equipe **Cobalto** ([ver membros](#2-membros-do-projeto)).

---

<div align="center">

**Desenvolvido com ‚ù§Ô∏è pela equipe Cobalto**

[‚¨Ü Voltar ao topo](#-avd---pipeline-de-bi-clim√°tico)

</div>
