# üçá AVD - Pipeline de BI Clim√°tico para Viticultura

## üìã √çndice

1. [Introdu√ß√£o e Objetivo](#1-introdu√ß√£o-e-objetivo)
2. [Membros do Projeto](#2-membros-do-projeto)
3. [Arquitetura do Pipeline](#3-arquitetura-do-pipeline)
4. [Estrutura do Reposit√≥rio](#4-estrutura-do-reposit√≥rio)
5. [Tecnologias Utilizadas](#5-tecnologias-utilizadas)
6. [Instru√ß√µes de Execu√ß√£o](#6-instru√ß√µes-de-execu√ß√£o)
7. [Notebooks do Projeto](#7-notebooks-do-projeto)
8. [Scripts Auxiliares](#8-scripts-auxiliares)
9. [Troubleshooting](#9-troubleshooting)
10. [Resultados e Conclus√µes](#10-resultados-e-conclus√µes)

---

## 1. Introdu√ß√£o e Objetivo

Este projeto implementa um pipeline de Business Intelligence (BI) para an√°lise e visualiza√ß√£o de dados meteorol√≥gicos do **INMET (Instituto Nacional de Meteorologia)**, focando no estado de **Pernambuco**, com √™nfase no **Vale do S√£o Francisco**.

### Objetivo Central

Aplicar t√©cnicas de **Agrupamento (Clustering) K-Means** para identificar **Padr√µes Clim√°ticos Chave** durante fases cr√≠ticas da videira, como a flora√ß√£o e a matura√ß√£o, utilizando dados agregados de temperatura, umidade, radia√ß√£o solar, vento, precipita√ß√£o e press√£o atmosf√©rica. O resultado deste agrupamento √© visualizado em dashboards interativos no **ThingsBoard**.

### Dados Processados

- **Per√≠odo:** 2020 a 2024
- **Esta√ß√µes:** Petrolina (A307) e Garanhuns (A322)
- **Frequ√™ncia:** Dados hor√°rios
- **Vari√°veis:** Temperatura, Umidade, Radia√ß√£o, Vento, Precipita√ß√£o, Press√£o

## 2. Membros do Projeto

| Nome | Usu√°rio |
| :--- | :--- |
| Lisa Matubara | `lm` |
| Luziane Santos | `lps` |
| Maria J√∫lia Peixoto | `mjpo` |
| Matheus Velame | `mvp2` |
| Paulo Rago | `prcr` |
| Tha√≠s Aguiar | `thcba` |

* **Disciplina:** An√°lise e Visualiza√ß√£o de Dados - 2025.2
* **Institui√ß√£o:** CESAR School

## 3. Arquitetura do Pipeline

A solu√ß√£o √© baseada em cont√™ineres **Docker** e orquestrada via **Docker Compose**, abrangendo as seguintes camadas:

| Servi√ßo | Fun√ß√£o Principal | Porta | URL de Acesso |
| :--- | :--- | :--- | :--- |
| **JupyterLab** | Ambiente de an√°lise, tratamento de dados e modelagem | `8888` | `http://localhost:8888` |
| **FastAPI** | Interface de ingest√£o dos dados brutos do INMET e integra√ß√£o com MinIO/S3 | `8000` | `http://localhost:8000` |
| **MinIO/S3** | Armazenamento de dados brutos e modelos | `9000` (API)<br>`9001` (Console) | `http://localhost:9001` |
| **Snowflake** | Estrutura√ß√£o de dados tratados (banco de dados cloud) | - | Configurado externamente |
| **MLFlow** | Registro e versionamento do modelo de K-Means e artefatos | `5000` | `http://localhost:5000` |
| **ThingsBoard** | Plataforma IoT para visualiza√ß√£o de dados e dashboards | `8090` | `http://localhost:8090` |

### Fluxo Geral do Pipeline

```mermaid
graph LR
    A[INMET CSV] --> B[FastAPI]
    B --> C[MinIO/S3]
    C --> D[Jupyter Notebook]
    D --> E[Snowflake]
    D --> F[MLFlow]
    E --> G[ThingsBoard]
    F --> G
```

1. **Ingest√£o:** Os dados brutos do INMET s√£o ingeridos via FastAPI e salvos no MinIO/S3.
2. **Tratamento:** O Jupyter Notebook processa os dados brutos, aplica limpeza e interpola√ß√£o temporal.
3. **Estrutura√ß√£o:** Os dados tratados s√£o carregados no Snowflake para armazenamento estruturado.
4. **Modelagem:** O notebook aplica K-Means para identificar padr√µes clim√°ticos e registra o modelo no MLFlow.
5. **Visualiza√ß√£o:** O ThingsBoard consome os resultados do agrupamento para gerar dashboards interativos.

## 4. Estrutura do Reposit√≥rio

```
AVD-projeto/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Dados brutos do INMET (CSV)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2020/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2021/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2022/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2023/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2024/
‚îÇ   ‚îî‚îÄ‚îÄ processed/               # Dados tratados (CSV)
‚îÇ       ‚îú‚îÄ‚îÄ petrolina_*_tratado.csv
‚îÇ       ‚îî‚îÄ‚îÄ garanhuns_*_tratado.csv
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_carregar_dados.ipynb          # Notebook explorat√≥rio
‚îÇ   ‚îú‚îÄ‚îÄ 01_tratamento_dados_inmet.ipynb  # Processamento completo e carga no Snowflake
‚îÇ   ‚îî‚îÄ‚îÄ 02_Modelagem_KMeans.ipynb         # Modelagem e clustering
‚îú‚îÄ‚îÄ fastapi/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # API de ingest√£o
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ etl_minio_to_snowflake.py    # ETL MinIO ‚Üí Snowflake
‚îÇ   ‚îú‚îÄ‚îÄ send_inmet_to_tb.py           # Envio de dados para ThingsBoard
‚îÇ   ‚îî‚îÄ‚îÄ test_pipeline.py              # Testes do pipeline
‚îú‚îÄ‚îÄ mlflow/
‚îÇ   ‚îî‚îÄ‚îÄ artifacts/                # Artefatos dos modelos
‚îú‚îÄ‚îÄ minio/
‚îÇ   ‚îî‚îÄ‚îÄ data/                     # Dados armazenados no MinIO
‚îú‚îÄ‚îÄ thingsboard/
‚îÇ   ‚îú‚îÄ‚îÄ data/                     # Banco de dados do ThingsBoard
‚îÇ   ‚îî‚îÄ‚îÄ logs/                     # Logs do ThingsBoard
‚îú‚îÄ‚îÄ docker-compose.yml            # Orquestra√ß√£o dos servi√ßos
‚îú‚îÄ‚îÄ Dockerfile.jupyter            # Dockerfile do Jupyter
‚îî‚îÄ‚îÄ README.md                     # Este arquivo
```

## 5. Tecnologias Utilizadas

### Backend e Infraestrutura
- **Docker & Docker Compose** - Containeriza√ß√£o e orquestra√ß√£o
- **FastAPI** - API REST para ingest√£o de dados
- **MinIO** - Armazenamento de objetos compat√≠vel com S3
- **Snowflake** - Data warehouse cloud
- **PostgreSQL** - Banco de dados do ThingsBoard

### An√°lise de Dados e Machine Learning
- **Python 3.11** - Linguagem principal
- **Pandas** - Manipula√ß√£o e an√°lise de dados
- **NumPy** - Computa√ß√£o num√©rica
- **Scikit-learn** - Machine Learning (K-Means)
- **JupyterLab** - Ambiente de desenvolvimento interativo

### MLOps e Versionamento
- **MLFlow** - Gerenciamento do ciclo de vida de modelos
- **Git** - Controle de vers√£o

### Visualiza√ß√£o e IoT
- **ThingsBoard** - Plataforma IoT para visualiza√ß√£o
- **Matplotlib** - Visualiza√ß√µes est√°ticas
- **Seaborn** - Visualiza√ß√µes estat√≠sticas (opcional)

## 6. Instru√ß√µes de Execu√ß√£o

### 6.1. Pr√©-requisitos

- **Docker** (vers√£o 20.10 ou superior)
- **Docker Compose** (vers√£o 2.0 ou superior)
- **Git** (para clonar o reposit√≥rio)
- **Conex√£o est√°vel com a internet** (para download de imagens Docker)
- **8GB de RAM** (recomendado)
- **10GB de espa√ßo em disco** (para dados e imagens)

### 6.2. Clonagem do Reposit√≥rio

```bash
git clone <git@github.com:aguiarth/AVD-projeto.git>
cd AVD-projeto
```

### 6.3. Subir a Infraestrutura

1. **Construir as imagens e iniciar os servi√ßos:**

```bash
docker-compose up -d --build
```

2. **Verificar se todos os servi√ßos est√£o rodando:**

```bash
docker-compose ps
```

Voc√™ deve ver todos os servi√ßos com status `Up`:
- `jupyter-uva`
- `thingsboard`
- `fastapi-clima`
- `minio`
- `mlflow_server`

3. **Acessar os servi√ßos:**

- **JupyterLab:** `http://localhost:8888` (sem token)
- **FastAPI:** `http://localhost:8000`
- **MinIO Console:** `http://localhost:9001` (usu√°rio: `admin`, senha: `admin12345`)
- **MLFlow:** `http://localhost:5000`
- **ThingsBoard:** `http://localhost:8090` (usu√°rio: `tenant@thingsboard.org`, senha: `tenant`)

### 6.4. Execu√ß√£o do Pipeline

#### Passo 1: Processamento dos Dados

1. **Acesse o JupyterLab:** `http://localhost:8888`

2. **Execute o notebook `01_tratamento_dados_inmet.ipynb`:**
   - Este notebook processa todos os arquivos CSV do INMET (2020-2024)
   - Aplica limpeza, interpola√ß√£o temporal e tratamento de valores faltantes
   - Salva os dados tratados em `/data/processed/`
   - **Carrega os dados diretamente no Snowflake**

   **Vari√°veis processadas:**
   - Temperatura do ar (¬∞C)
   - Umidade relativa (%)
   - Radia√ß√£o global (kJ/m¬≤) - quando dispon√≠vel
   - Velocidade do vento (m/s)
   - Precipita√ß√£o (mm)
   - Press√£o atmosf√©rica (mB)

3. **Para explora√ß√£o r√°pida, use o notebook `01_carregar_dados.ipynb`:**
   - Permite visualizar e explorar um arquivo espec√≠fico
   - Usa a mesma fun√ß√£o de processamento do notebook principal

#### Passo 2: Modelagem K-Means

1. **Execute o notebook `02_Modelagem_KMeans.ipynb`:**
   - Carrega os dados tratados (de `/data/processed/` ou Snowflake)
   - Agrega dados por semana
   - Trata outliers
   - Aplica normaliza√ß√£o (StandardScaler)
   - Treina o modelo K-Means
   - Avalia o modelo (silhouette score)
   - Registra o modelo no MLFlow

2. **Visualizar o modelo no MLFlow:**
   - Acesse `http://localhost:5000`
   - Navegue at√© o experimento "K-Means Clustering"
   - Visualize m√©tricas, par√¢metros e artefatos

#### Passo 3: Visualiza√ß√£o no ThingsBoard

1. **Acesse o ThingsBoard:** `http://localhost:8090`
   - Credenciais padr√£o: `tenant@thingsboard.org` / `tenant`

2. **Configure dispositivos e dashboards:**
   - Crie dispositivos para cada esta√ß√£o (Petrolina, Garanhuns)
   - Use o script `scripts/send_inmet_to_tb.py` para enviar dados
   - Crie dashboards para visualizar os clusters identificados

## 7. Notebooks do Projeto

### `01_carregar_dados.ipynb`

**Prop√≥sito:** Notebook explorat√≥rio para visualiza√ß√£o e an√°lise r√°pida de dados.

**Funcionalidades:**
- Lista arquivos CSV dispon√≠veis
- Processa um arquivo espec√≠fico usando a fun√ß√£o `processar_inmet()`
- Exibe estat√≠sticas descritivas
- Verifica valores faltantes
- Visualiza amostras dos dados

**Quando usar:** Para explora√ß√£o inicial dos dados ou an√°lise de um arquivo espec√≠fico.

### `01_tratamento_dados_inmet.ipynb`

**Prop√≥sito:** Processamento completo de todos os arquivos do INMET e carga no Snowflake.

**Funcionalidades:**
- Processa todos os arquivos CSV (2020-2024, Petrolina e Garanhuns)
- Aplica fun√ß√£o `processar_inmet()` padronizada
- Interpola valores faltantes usando m√©todo temporal
- Remove colunas 100% vazias (ex: radia√ß√£o quando ausente)
- Cria features auxiliares (hora_num, mes)
- Salva dados tratados em CSV
- **Carrega dados no Snowflake**

**Tratamento aplicado:**
- Convers√£o de v√≠rgula para ponto decimal
- Padroniza√ß√£o de formato de hora
- Cria√ß√£o de √≠ndice datetime
- Interpola√ß√£o temporal de valores faltantes
- Preenchimento de bordas (ffill/bfill)

**Quando usar:** Para processar todos os dados e preparar para modelagem.

### `02_Modelagem_KMeans.ipynb`

**Prop√≥sito:** Modelagem de clustering para identificar padr√µes clim√°ticos.

**Funcionalidades:**
- Carrega dados tratados
- Agrega√ß√£o semanal dos dados hor√°rios
- Tratamento de outliers
- Normaliza√ß√£o com StandardScaler
- Treinamento de K-Means
- Avalia√ß√£o com silhouette score
- Visualiza√ß√£o dos clusters
- Registro no MLFlow

**Quando usar:** Ap√≥s o processamento dos dados, para identificar padr√µes clim√°ticos.

## 8. Scripts Auxiliares

### `scripts/etl_minio_to_snowflake.py`

Script para extrair dados do MinIO e carregar no Snowflake.

**Uso:**
```bash
python scripts/etl_minio_to_snowflake.py
```

### `scripts/send_inmet_to_tb.py`

Script para enviar dados processados para o ThingsBoard.

**Uso:**
```bash
python scripts/send_inmet_to_tb.py
```

### `scripts/test_pipeline.py`

Script de testes para validar o pipeline completo.

**Uso:**
```bash
python scripts/test_pipeline.py
```

## 9. Troubleshooting

### Problema: Servi√ßos n√£o iniciam

**Solu√ß√£o:**
```bash
# Verificar logs
docker-compose logs

# Reiniciar servi√ßos
docker-compose restart

# Reconstruir imagens
docker-compose up -d --build --force-recreate
```

### Problema: Porta j√° em uso

**Solu√ß√£o:**
- Verifique se outra aplica√ß√£o est√° usando a porta
- Altere a porta no `docker-compose.yml` se necess√°rio
- Use `docker-compose down` antes de subir novamente

### Problema: Erro ao processar dados

**Solu√ß√£o:**
- Verifique se os arquivos CSV est√£o em `/data/raw/` com a estrutura correta
- Confirme que o encoding √© `latin1`
- Verifique os logs do Jupyter: `docker-compose logs jupyterlab`

### Problema: Snowflake n√£o conecta

**Solu√ß√£o:**
- Verifique as credenciais no notebook `01_tratamento_dados_inmet.ipynb`
- Confirme que o Snowflake est√° acess√≠vel
- Verifique a configura√ß√£o de rede/firewall

### Problema: MLFlow n√£o salva modelos

**Solu√ß√£o:**
- Verifique se o volume `./mlflow` est√° montado corretamente
- Confirme permiss√µes de escrita no diret√≥rio
- Verifique logs: `docker-compose logs mlflow`

### Comandos √öteis

```bash
# Parar todos os servi√ßos
docker-compose down

# Parar e remover volumes
docker-compose down -v

# Ver logs de um servi√ßo espec√≠fico
docker-compose logs -f jupyterlab

# Executar comando em um container
docker-compose exec jupyterlab bash

# Limpar recursos n√£o utilizados
docker system prune -a
```

## 10. Resultados e Conclus√µes

### Dados Processados

- **Total de registros:** ~87.000 registros hor√°rios (por ano)
- **Per√≠odo:** 2020-2024
- **Esta√ß√µes:** 2 (Petrolina e Garanhuns)
- **Vari√°veis clim√°ticas:** 6 principais

### Modelo K-Means

- **M√©todo:** Clustering n√£o-supervisionado
- **Features:** Agrega√ß√µes semanais de vari√°veis clim√°ticas
- **Avalia√ß√£o:** Silhouette score
- **Versionamento:** MLFlow

### Visualiza√ß√£o

- **Plataforma:** ThingsBoard
- **Dashboards:** Padr√µes clim√°ticos por cluster
- **Interatividade:** Filtros por per√≠odo, esta√ß√£o e vari√°vel

### Relat√≥rio T√©cnico

O relat√≥rio final em PDF, contendo a arquitetura, metodologia, resultados e conclus√µes, ser√° enviado junto da entrega.

## üìö Refer√™ncias

- [INMET - Instituto Nacional de Meteorologia](https://portal.inmet.gov.br/)
- [ThingsBoard - Documenta√ß√£o](https://thingsboard.io/docs/)
- [MLFlow - Documenta√ß√£o](https://www.mlflow.org/docs/latest/index.html)
- [Snowflake - Documenta√ß√£o](https://docs.snowflake.com/)
- [Scikit-learn K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)

## üìù Licen√ßa

Este projeto √© desenvolvido para fins acad√™micos no contexto da disciplina de An√°lise e Visualiza√ß√£o de Dados da CESAR School.

## ü§ù Equipe

Este √© um projeto acad√™mico desenvolvido pela equipe [Cobalto](#2-membros-do-projeto).
