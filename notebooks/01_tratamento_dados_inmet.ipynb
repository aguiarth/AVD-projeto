{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af69ba04-d958-4c82-bac4-517013ffc042",
   "metadata": {},
   "source": [
    "Importa√ß√µes e configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d47a8561-13ce-4ef0-a776-d45a7aef1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conex√£o MinIO estabelecida.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# NOVAS IMPORTA√á√ïES NECESS√ÅRIAS PARA MINIO/S3\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import io # Para manipular streams de bytes lidos do MinIO\n",
    "\n",
    "# --- CONFIGURA√á√ÉO DE AMBIENTE E CAMINHOS ---\n",
    "\n",
    "# Caminho de sa√≠da (continua sendo local)\n",
    "BASE_PROC = Path(\"/home/jovyan/data/processed\")\n",
    "BASE_PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- CONFIGURA√á√ÉO DO MINIO (Data Lake) ---\n",
    "# O endpoint 'minio:9000' √© o nome do servi√ßo na rede Docker Compose\n",
    "MINIO_ENDPOINT = \"minio:9000\" \n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"admin12345\"\n",
    "RAW_BUCKET = \"inmet-raw\"\n",
    "# Prefixo que voc√™ usou no script de upload\n",
    "RAW_PREFIX = \"raw/\" \n",
    "\n",
    "# Cria o cliente MinIO (executado dentro do cont√™iner Jupyter)\n",
    "try:\n",
    "    minio_client = Minio(\n",
    "        MINIO_ENDPOINT,\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY,\n",
    "        secure=False,\n",
    "    )\n",
    "    print(\"‚úÖ Conex√£o MinIO estabelecida.\")\n",
    "    if not minio_client.bucket_exists(RAW_BUCKET):\n",
    "        print(f\"‚ùå Erro: O bucket '{RAW_BUCKET}' n√£o existe. Verifique o servi√ßo MinIO.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao conectar ao MinIO: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fddba76-96a2-4214-a85e-6ef9751efb49",
   "metadata": {},
   "source": [
    "Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22312fd7-7e92-4909-b70b-abb447136bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_inmet(file_content: bytes, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processa o conte√∫do de um arquivo CSV bruto do INMET, lendo-o de um stream de bytes (MinIO/S3).\n",
    "    Aplica limpeza de dados, convers√£o de tipos, tratamento de faltantes por interpola√ß√£o.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìÑ Lendo arquivo: {file_name}\")\n",
    "    \n",
    "    # 1) LER CSV DO STREAM DE BYTES\n",
    "    df = pd.read_csv(\n",
    "        io.BytesIO(file_content), # <--- MUDAN√áA AQUI: L√™ do stream de bytes\n",
    "        sep=';',\n",
    "        skiprows=8,           # pula bloco de metadados\n",
    "        encoding='latin1',\n",
    "        low_memory=False\n",
    "    )\n",
    "    \n",
    "    # 2) Renomear colunas de interesse (s√≥ as que existirem)\n",
    "    mapa_renome = {\n",
    "        'Data': 'data',\n",
    "        'Hora UTC': 'hora',\n",
    "        'TEMPERATURA DO AR - BULBO SECO, HORARIA (¬∞C)': 'temp_ar',\n",
    "        'UMIDADE RELATIVA DO AR, HORARIA (%)': 'umidade',\n",
    "        'RADIACAO GLOBAL (Kj/m¬≤)': 'radiacao',\n",
    "        'RADIACAO GLOBAL (kJ/m¬≤)': 'radiacao',\n",
    "        'RADIACAO GLOBAL': 'radiacao',\n",
    "        'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)': 'precipitacao',\n",
    "        'VENTO, VELOCIDADE HORARIA (m/s)': 'vento_vel',\n",
    "        'VENTO, DIRE√á√ÉO HORARIA (gr) (¬∞ (gr))': 'vento_dir',\n",
    "        'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)': 'pressao'\n",
    "    }\n",
    "    df = df.rename(columns={orig: novo for orig, novo in mapa_renome.items() if orig in df.columns})\n",
    "\n",
    "    # 3) Garante que data e hora existem\n",
    "    if 'data' not in df.columns or 'hora' not in df.columns:\n",
    "        raise ValueError(\"Colunas 'Data' e/ou 'Hora UTC' n√£o foram encontradas ap√≥s o rename.\")\n",
    "\n",
    "    # 4) Padronizar 'data' e 'hora' como string\n",
    "    df['data'] = df['data'].astype(str).str.strip()\n",
    "    df['hora'] = df['hora'].astype(str).str.strip()\n",
    "\n",
    "    # 5) Remover ' UTC' da hora, se existir\n",
    "    df['hora'] = df['hora'].str.replace(' UTC', '', regex=False)\n",
    "\n",
    "    # 6) Se hora estiver como 0, 300, 1200 etc, padronizar para HH:MM\n",
    "    mascara_numerica = df['hora'].str.fullmatch(r'\\d{1,4}')\n",
    "    df.loc[mascara_numerica, 'hora'] = (\n",
    "        df.loc[mascara_numerica, 'hora']\n",
    "          .str.zfill(4)\n",
    "          .str[:2] + ':' + df.loc[mascara_numerica, 'hora'].str.zfill(4).str[2:]\n",
    "    )\n",
    "\n",
    "    # 7) Criar coluna datetime (deixa o pandas inferir formato)\n",
    "    df['datetime'] = pd.to_datetime(\n",
    "        df['data'] + ' ' + df['hora'],\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Remover linhas onde datetime n√£o p√¥de ser montado\n",
    "    df = df.dropna(subset=['datetime'])\n",
    "    df = df.set_index('datetime')\n",
    "\n",
    "    # 8) Selecionar colunas de interesse (as que existirem)\n",
    "    colunas_desejadas = ['hora', 'temp_ar', 'umidade', 'radiacao',\n",
    "                         'vento_vel', 'precipitacao', 'pressao']\n",
    "    colunas_presentes = [c for c in colunas_desejadas if c in df.columns]\n",
    "    df = df[colunas_presentes]\n",
    "\n",
    "    # 9) Converter vari√°veis num√©ricas para float\n",
    "    variaveis_numericas = [c for c in colunas_presentes if c != 'hora']\n",
    "\n",
    "    for col in variaveis_numericas:\n",
    "        df[col] = df[col].astype(str).str.replace(',', '.', regex=False).str.strip()\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 10) Remover colunas que est√£o 100% NaN \n",
    "    colunas_todas_nan = [c for c in variaveis_numericas if df[c].isna().sum() == len(df)]\n",
    "    if colunas_todas_nan:\n",
    "        print(\"   ‚ö† Removendo colunas 100% NaN:\", colunas_todas_nan)\n",
    "        df = df.drop(columns=colunas_todas_nan)\n",
    "        variaveis_numericas = [c for c in variaveis_numericas if c not in colunas_todas_nan]\n",
    "\n",
    "    # 11) Interpolar valores faltantes ao longo do tempo\n",
    "    if variaveis_numericas:\n",
    "        df[variaveis_numericas] = df[variaveis_numericas].interpolate(\n",
    "            method='time'\n",
    "        )\n",
    "\n",
    "        # Opcional: preencher pontas (in√≠cio/fim) com ffill/bfill\n",
    "        df[variaveis_numericas] = df[variaveis_numericas].ffill().bfill()\n",
    "\n",
    "    # 12) Remover apenas linhas em que TODAS as vari√°veis num√©ricas s√£o NaN (caso sobrem)\n",
    "    if variaveis_numericas:\n",
    "        mask_all_nan = df[variaveis_numericas].isna().all(axis=1)\n",
    "        n_all_nan = mask_all_nan.sum()\n",
    "        if n_all_nan > 0:\n",
    "            print(f\"   ‚ö† Removendo {n_all_nan} linhas 100% NaN nas vari√°veis num√©ricas.\")\n",
    "            df = df[~mask_all_nan]\n",
    "\n",
    "    # 13) Criar features auxiliares\n",
    "    df['hora_num'] = df.index.hour\n",
    "    df['mes'] = df.index.month\n",
    "\n",
    "    print(f\"   ‚úÖ Registros ap√≥s tratamento: {len(df)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5347a54b-dd10-4f8e-9542-a5f6b54c1f2a",
   "metadata": {},
   "source": [
    "Loop para processar Petrolina e Garanhuns (2020 a 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dffff4fb-f874-4f54-ad59-98c61df4e7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anos a processar: ['2020', '2021', '2022', '2023', '2024']\n",
      "Cidades a processar: ['PETROLINA', 'GARANHUNS']\n",
      "\n",
      "üì• Baixando raw/2020/INMET_NE_PE_A307_PETROLINA_01-01-2020_A_31-12-2020.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A307_PETROLINA_01-01-2020_A_31-12-2020.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8784\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/petrolina_2020_tratado.csv\n",
      "   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: DADOS_INMET_HORARIO).\n",
      "\n",
      "üì• Baixando raw/2020/INMET_NE_PE_A322_GARANHUNS_01-01-2020_A_31-12-2020.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A322_GARANHUNS_01-01-2020_A_31-12-2020.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8784\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/garanhuns_2020_tratado.csv\n",
      "   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: DADOS_INMET_HORARIO).\n",
      "\n",
      "üì• Baixando raw/2021/INMET_NE_PE_A307_PETROLINA_01-01-2021_A_31-12-2021.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A307_PETROLINA_01-01-2021_A_31-12-2021.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/petrolina_2021_tratado.csv\n",
      "   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: DADOS_INMET_HORARIO).\n",
      "\n",
      "üì• Baixando raw/2021/INMET_NE_PE_A322_GARANHUNS_01-01-2021_A_31-12-2021.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A322_GARANHUNS_01-01-2021_A_31-12-2021.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/garanhuns_2021_tratado.csv\n",
      "   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: DADOS_INMET_HORARIO).\n",
      "\n",
      "üì• Baixando raw/2022/INMET_NE_PE_A307_PETROLINA_01-01-2022_A_31-12-2022.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A307_PETROLINA_01-01-2022_A_31-12-2022.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/petrolina_2022_tratado.csv\n",
      "   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: DADOS_INMET_HORARIO).\n",
      "\n",
      "üì• Baixando raw/2022/INMET_NE_PE_A322_GARANHUNS_01-01-2022_A_31-12-2022.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A322_GARANHUNS_01-01-2022_A_31-12-2022.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/garanhuns_2022_tratado.csv\n",
      "   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: DADOS_INMET_HORARIO).\n",
      "\n",
      "üì• Baixando raw/2023/INMET_NE_PE_A307_PETROLINA_01-01-2023_A_31-12-2023.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A307_PETROLINA_01-01-2023_A_31-12-2023.CSV\n",
      "   ‚ö† Removendo colunas 100% NaN: ['radiacao']\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/petrolina_2023_tratado.csv\n",
      "   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: DADOS_INMET_HORARIO).\n",
      "\n",
      "üì• Baixando raw/2023/INMET_NE_PE_A322_GARANHUNS_01-01-2023_A_31-12-2023.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A322_GARANHUNS_01-01-2023_A_31-12-2023.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/garanhuns_2023_tratado.csv\n",
      "   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: DADOS_INMET_HORARIO).\n",
      "\n",
      "üì• Baixando raw/2024/INMET_NE_PE_A307_PETROLINA_01-01-2024_A_31-12-2024.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A307_PETROLINA_01-01-2024_A_31-12-2024.CSV\n",
      "   ‚ö† Removendo colunas 100% NaN: ['radiacao']\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8784\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/petrolina_2024_tratado.csv\n",
      "   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: DADOS_INMET_HORARIO).\n",
      "\n",
      "üì• Baixando raw/2024/INMET_NE_PE_A322_GARANHUNS_01-01-2024_A_31-12-2024.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A322_GARANHUNS_01-01-2024_A_31-12-2024.CSV\n",
      "   ‚ö† Removendo colunas 100% NaN: ['precipitacao']\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8784\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/garanhuns_2024_tratado.csv\n",
      "   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: DADOS_INMET_HORARIO).\n"
     ]
    }
   ],
   "source": [
    "# Defini√ß√£o dos anos e cidades para itera√ß√£o\n",
    "anos = ['2020', '2021', '2022', '2023', '2024']\n",
    "cidades = ['PETROLINA', 'GARANHUNS']\n",
    "print(f\"Anos a processar: {anos}\")\n",
    "print(f\"Cidades a processar: {cidades}\")\n",
    "\n",
    "for ano in anos:\n",
    "    for cidade in cidades:\n",
    "        estacao_id = cidade.lower()\n",
    "        \n",
    "        # 1. Lista Objetos no MinIO\n",
    "        # Busca por prefixo: raw/<ano>/\n",
    "        search_prefix = f\"{RAW_PREFIX}{ano}/\"\n",
    "        \n",
    "        objetos_minio = minio_client.list_objects(\n",
    "            RAW_BUCKET, \n",
    "            prefix=search_prefix, \n",
    "            recursive=True\n",
    "        )\n",
    "\n",
    "        # Filtra os objetos que cont√™m o nome da cidade e terminam com .CSV\n",
    "        arquivos_encontrados = [obj for obj in objetos_minio \n",
    "                                if cidade.upper() in obj.object_name.upper() and obj.object_name.endswith(\".CSV\")]\n",
    "\n",
    "        if not arquivos_encontrados:\n",
    "            print(f\"\\nüö´ Nenhum arquivo encontrado para {cidade} em {ano} no MinIO com prefixo {search_prefix}\")\n",
    "            continue\n",
    "\n",
    "        for obj in arquivos_encontrados:\n",
    "            response = None \n",
    "            try:\n",
    "                # 2. Baixa o conte√∫do do arquivo do MinIO (como bytes)\n",
    "                print(f\"\\nüì• Baixando {obj.object_name}...\")\n",
    "                response = minio_client.get_object(RAW_BUCKET, obj.object_name)\n",
    "                # L√™ o conte√∫do completo do stream\n",
    "                file_content = response.read()\n",
    "                \n",
    "                # Pega apenas o nome do arquivo para usar no print de log\n",
    "                file_name_short = obj.object_name.split('/')[-1]\n",
    "\n",
    "                # 3. Processamento e Limpeza (Fun√ß√£o modificada recebe bytes)\n",
    "                df_tratado = processar_inmet(file_content, file_name_short)\n",
    "                \n",
    "                # --- PREPARA√á√ÉO PARA O SNOWFLAKE (Mantida do seu original) ---\n",
    "                \n",
    "                # 3a. Adiciona a coluna ESTACAO_ID\n",
    "                df_tratado['ESTACAO_ID'] = estacao_id\n",
    "                \n",
    "                # 3b. Renomeia colunas para bater com o DDL final\n",
    "                mapa_cols_snowflake = {\n",
    "                    'pressao': 'PRESSAO',\n",
    "                    'radiacao': 'RADIACAO',\n",
    "                    'vento_vel': 'VENTO_VEL',\n",
    "                    'precipitacao': 'PRECIPITACAO',\n",
    "                    'temp_ar': 'TEMP_AR',\n",
    "                    'umidade': 'UMIDADE',\n",
    "                    'hora_num': 'HORA_NUM',\n",
    "                    'mes': 'MES',\n",
    "                    'hora': 'HORA'\n",
    "                }\n",
    "\n",
    "                df_final = df_tratado.reset_index().rename(columns=mapa_cols_snowflake)\n",
    "            \n",
    "                if 'index' in df_final.columns:\n",
    "                    df_final = df_final.drop(columns=['index'])\n",
    "\n",
    "                df_final.rename(columns={'datetime': 'DATETIME'}, inplace=True)\n",
    "                \n",
    "                # 3c. Garante a ordem das colunas\n",
    "                colunas_ddl = [\n",
    "                    'DATETIME', 'ESTACAO_ID', 'TEMP_AR', 'UMIDADE', 'PRESSAO', \n",
    "                    'VENTO_VEL', 'RADIACAO', 'PRECIPITACAO', 'HORA_NUM', 'MES', 'HORA'\n",
    "                ]\n",
    "                \n",
    "                colunas_existentes = [c for c in colunas_ddl if c in df_final.columns]\n",
    "                df_final = df_final[colunas_existentes]\n",
    "\n",
    "\n",
    "                # 4. Salva CSV Localmente (como arquivo processado)\n",
    "                nome_saida = f\"{estacao_id}_{ano}_tratado.csv\"\n",
    "                caminho_saida = BASE_PROC / nome_saida\n",
    "                # Salva o dataframe tratado, usando o √≠ndice datetime\n",
    "                df_tratado.to_csv(caminho_saida, index=True, index_label='DATETIME') \n",
    "                print(f\"   üíæ Salvo em: {caminho_saida}\")\n",
    "                \n",
    "                # 5. Carga para o Snowflake\n",
    "                # carregar_para_snowflake(df_final, SNOWFLAKE_TABLE)\n",
    "                print(f\"   ‚û°Ô∏è DataFrame pronto para carga no Snowflake (Tabela: {SNOWFLAKE_TABLE}).\")\n",
    "\n",
    "\n",
    "            except S3Error as e:\n",
    "                print(f\"   ‚ùå Erro S3 ao processar {obj.object_name}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Erro ao processar {obj.object_name}: {e}\")\n",
    "            finally:\n",
    "                # √â crucial fechar a conex√£o do stream do MinIO ap√≥s o uso\n",
    "                if response:\n",
    "                    response.close()\n",
    "                    response.release_conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4007a76",
   "metadata": {},
   "source": [
    "Resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83e97da1-d3a9-4202-8c31-0032e0b49ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foram encontrados 10 arquivos tratados.\n",
      "\n",
      "üìÑ Arquivo: petrolina_2020_tratado.csv\n",
      "   ‚û§ Linhas: 8784\n",
      "   ‚û§ NaNs por coluna:\n",
      "hora            0\n",
      "temp_ar         0\n",
      "umidade         0\n",
      "radiacao        0\n",
      "vento_vel       0\n",
      "precipitacao    0\n",
      "pressao         0\n",
      "hora_num        0\n",
      "mes             0\n",
      "ESTACAO_ID      0\n",
      "------------------------------------------------------------\n",
      "üìÑ Arquivo: petrolina_2022_tratado.csv\n",
      "   ‚û§ Linhas: 8760\n",
      "   ‚û§ NaNs por coluna:\n",
      "hora            0\n",
      "temp_ar         0\n",
      "umidade         0\n",
      "radiacao        0\n",
      "vento_vel       0\n",
      "precipitacao    0\n",
      "pressao         0\n",
      "hora_num        0\n",
      "mes             0\n",
      "ESTACAO_ID      0\n",
      "------------------------------------------------------------\n",
      "üìÑ Arquivo: garanhuns_2024_tratado.csv\n",
      "   ‚û§ Linhas: 8784\n",
      "   ‚û§ NaNs por coluna:\n",
      "hora          0\n",
      "temp_ar       0\n",
      "umidade       0\n",
      "radiacao      0\n",
      "vento_vel     0\n",
      "pressao       0\n",
      "hora_num      0\n",
      "mes           0\n",
      "ESTACAO_ID    0\n",
      "------------------------------------------------------------\n",
      "üìÑ Arquivo: garanhuns_2023_tratado.csv\n",
      "   ‚û§ Linhas: 8760\n",
      "   ‚û§ NaNs por coluna:\n",
      "hora            0\n",
      "temp_ar         0\n",
      "umidade         0\n",
      "radiacao        0\n",
      "vento_vel       0\n",
      "precipitacao    0\n",
      "pressao         0\n",
      "hora_num        0\n",
      "mes             0\n",
      "ESTACAO_ID      0\n",
      "------------------------------------------------------------\n",
      "üìÑ Arquivo: petrolina_2024_tratado.csv\n",
      "   ‚û§ Linhas: 8784\n",
      "   ‚û§ NaNs por coluna:\n",
      "hora            0\n",
      "temp_ar         0\n",
      "umidade         0\n",
      "vento_vel       0\n",
      "precipitacao    0\n",
      "pressao         0\n",
      "hora_num        0\n",
      "mes             0\n",
      "ESTACAO_ID      0\n",
      "------------------------------------------------------------\n",
      "üìÑ Arquivo: petrolina_2023_tratado.csv\n",
      "   ‚û§ Linhas: 8760\n",
      "   ‚û§ NaNs por coluna:\n",
      "hora            0\n",
      "temp_ar         0\n",
      "umidade         0\n",
      "vento_vel       0\n",
      "precipitacao    0\n",
      "pressao         0\n",
      "hora_num        0\n",
      "mes             0\n",
      "ESTACAO_ID      0\n",
      "------------------------------------------------------------\n",
      "üìÑ Arquivo: garanhuns_2022_tratado.csv\n",
      "   ‚û§ Linhas: 8760\n",
      "   ‚û§ NaNs por coluna:\n",
      "hora            0\n",
      "temp_ar         0\n",
      "umidade         0\n",
      "radiacao        0\n",
      "vento_vel       0\n",
      "precipitacao    0\n",
      "pressao         0\n",
      "hora_num        0\n",
      "mes             0\n",
      "ESTACAO_ID      0\n",
      "------------------------------------------------------------\n",
      "üìÑ Arquivo: garanhuns_2021_tratado.csv\n",
      "   ‚û§ Linhas: 8760\n",
      "   ‚û§ NaNs por coluna:\n",
      "hora            0\n",
      "temp_ar         0\n",
      "umidade         0\n",
      "radiacao        0\n",
      "vento_vel       0\n",
      "precipitacao    0\n",
      "pressao         0\n",
      "hora_num        0\n",
      "mes             0\n",
      "ESTACAO_ID      0\n",
      "------------------------------------------------------------\n",
      "üìÑ Arquivo: petrolina_2021_tratado.csv\n",
      "   ‚û§ Linhas: 8760\n",
      "   ‚û§ NaNs por coluna:\n",
      "hora            0\n",
      "temp_ar         0\n",
      "umidade         0\n",
      "radiacao        0\n",
      "vento_vel       0\n",
      "precipitacao    0\n",
      "pressao         0\n",
      "hora_num        0\n",
      "mes             0\n",
      "ESTACAO_ID      0\n",
      "------------------------------------------------------------\n",
      "üìÑ Arquivo: garanhuns_2020_tratado.csv\n",
      "   ‚û§ Linhas: 8784\n",
      "   ‚û§ NaNs por coluna:\n",
      "hora            0\n",
      "temp_ar         0\n",
      "umidade         0\n",
      "radiacao        0\n",
      "vento_vel       0\n",
      "precipitacao    0\n",
      "pressao         0\n",
      "hora_num        0\n",
      "mes             0\n",
      "ESTACAO_ID      0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "processed_path = BASE_PROC\n",
    "\n",
    "arquivos_tratados = list(processed_path.glob(\"*_tratado.csv\"))\n",
    "\n",
    "print(\"Foram encontrados\", len(arquivos_tratados), \"arquivos tratados.\\n\")\n",
    "\n",
    "for arquivo in arquivos_tratados:\n",
    "    df = pd.read_csv(arquivo, index_col=0, parse_dates=True)\n",
    "    \n",
    "    print(\"üìÑ Arquivo:\", arquivo.name)\n",
    "    print(\"   ‚û§ Linhas:\", len(df))\n",
    "    print(\"   ‚û§ NaNs por coluna:\")\n",
    "    print(df.isna().sum().to_string())\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21afadf-9014-456a-a43a-20bce3879a33",
   "metadata": {},
   "source": [
    "Nota:\n",
    "\n",
    "Para lidar com valores clim√°ticos faltantes nos arquivos do INMET, utilizamos a t√©cnica de interpola√ß√£o temporal, que √© um m√©todo recomendado internacionalmente para reconstru√ß√£o de s√©ries meteorol√≥gicas. Em termos simples, a interpola√ß√£o preenche lacunas usando os valores anteriores e posteriores como refer√™ncia. Por exemplo: se √†s 10h a temperatura √© 22¬∞C e √†s 12h √© 24¬∞C, a interpola√ß√£o estima que √†s 11h ela seria aproximadamente 23¬∞C. Isso evita descartar horas inteiras ‚Äî o que reduziria drasticamente a qualidade do conjunto de dados ‚Äî e mant√©m a continuidade natural do clima, que muda de forma gradual ao longo do tempo. Esse procedimento foi aplicado usando df.interpolate(method='time'), que utiliza o √≠ndice de datas do pr√≥prio dataframe para estimar valores de forma coerente com a evolu√ß√£o temporal. O uso de interpola√ß√£o em dados clim√°ticos √© amplamente adotado por institui√ß√µes como o NOAA ‚Äì National Oceanic and Atmospheric Administration, que afirma que a interpola√ß√£o linear √© adequada para preenchimento de falhas curtas em s√©ries meteorol√≥gicas cont√≠nuas (NOAA, Climate Data Interpolation Guidelines, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40053316",
   "metadata": {},
   "source": [
    "CONFIGURA√á√ÉO POSTGRESQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa7bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configura√ß√µes do Snowflake definidas.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# CONFIGURA√á√ÉO POSTGRESQL\n",
    "# ============================\n",
    "DB_CONFIG = {\n",
    "    'host': 'postgres',\n",
    "    'port': 5432,\n",
    "    'database': 'dados_clima',\n",
    "    'user': 'inmet_user',\n",
    "    'password': 'inmet123'\n",
    "}\n",
    "\n",
    "# Criar engine do SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@\"\n",
    "    f\"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configura√ß√£o PostgreSQL definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de389ebe",
   "metadata": {},
   "source": [
    "FUN√á√ÉO PARA CARREGAR DADOS NO POSTGRESQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f4c8a-ca59-4034-b9fd-054a300b87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FUN√á√ÉO PARA CARREGAR DADOS NO POSTGRESQL\n",
    "# ==============================================================================\n",
    "\n",
    "def carregar_para_postgres(df: pd.DataFrame, nome_tabela: str):\n",
    "    \"\"\"\n",
    "    Carrega um DataFrame Pandas para a tabela PostgreSQL especificada.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüêò Salvando dados na tabela {nome_tabela} do PostgreSQL...\")\n",
    "    \n",
    "    try:\n",
    "        # Salvar no PostgreSQL\n",
    "        df.to_sql(\n",
    "            name=nome_tabela,\n",
    "            con=engine,\n",
    "            if_exists='append',      # 'replace' para substituir, 'append' para adicionar\n",
    "            index=False,             # salva o √≠ndice datetime\n",
    "            index_label='datetime'\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úÖ Carga conclu√≠da! {len(df)} linhas salvas.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ERRO ao carregar: {e}\")\n",
    "        print(\"   Primeiras linhas do DataFrame com erro:\")\n",
    "        print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3d845",
   "metadata": {},
   "source": [
    "LOOP PARA PROCESSAR, SALVAR CSV LOCALMENTE E CARREGAR NO POSTGRESQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e5443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Tabela DADOS_INMET_HORARIO truncada com sucesso no Snowflake.\n",
      "\n",
      "üì• Baixando raw/2020/INMET_NE_PE_A307_PETROLINA_01-01-2020_A_31-12-2020.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A307_PETROLINA_01-01-2020_A_31-12-2020.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8784\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/petrolina_2020_tratado.csv\n",
      "\n",
      "‚ùÑÔ∏è Conectando ao Snowflake para carregar a tabela DADOS_INMET_HORARIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/pandas_tools.py:407: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  [pandas.api.types.is_datetime64tz_dtype(df[c]) for c in df.columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga conclu√≠da. Linhas carregadas: 8784\n",
      "   ‚úÖ Arquivos enviados: 1 chunk(s).\n",
      "\n",
      "üì• Baixando raw/2020/INMET_NE_PE_A322_GARANHUNS_01-01-2020_A_31-12-2020.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A322_GARANHUNS_01-01-2020_A_31-12-2020.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8784\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/garanhuns_2020_tratado.csv\n",
      "\n",
      "‚ùÑÔ∏è Conectando ao Snowflake para carregar a tabela DADOS_INMET_HORARIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/pandas_tools.py:407: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  [pandas.api.types.is_datetime64tz_dtype(df[c]) for c in df.columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga conclu√≠da. Linhas carregadas: 8784\n",
      "   ‚úÖ Arquivos enviados: 1 chunk(s).\n",
      "\n",
      "üì• Baixando raw/2021/INMET_NE_PE_A307_PETROLINA_01-01-2021_A_31-12-2021.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A307_PETROLINA_01-01-2021_A_31-12-2021.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/petrolina_2021_tratado.csv\n",
      "\n",
      "‚ùÑÔ∏è Conectando ao Snowflake para carregar a tabela DADOS_INMET_HORARIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/pandas_tools.py:407: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  [pandas.api.types.is_datetime64tz_dtype(df[c]) for c in df.columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga conclu√≠da. Linhas carregadas: 8760\n",
      "   ‚úÖ Arquivos enviados: 1 chunk(s).\n",
      "\n",
      "üì• Baixando raw/2021/INMET_NE_PE_A322_GARANHUNS_01-01-2021_A_31-12-2021.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A322_GARANHUNS_01-01-2021_A_31-12-2021.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/garanhuns_2021_tratado.csv\n",
      "\n",
      "‚ùÑÔ∏è Conectando ao Snowflake para carregar a tabela DADOS_INMET_HORARIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/pandas_tools.py:407: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  [pandas.api.types.is_datetime64tz_dtype(df[c]) for c in df.columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga conclu√≠da. Linhas carregadas: 8760\n",
      "   ‚úÖ Arquivos enviados: 1 chunk(s).\n",
      "\n",
      "üì• Baixando raw/2022/INMET_NE_PE_A307_PETROLINA_01-01-2022_A_31-12-2022.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A307_PETROLINA_01-01-2022_A_31-12-2022.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/petrolina_2022_tratado.csv\n",
      "\n",
      "‚ùÑÔ∏è Conectando ao Snowflake para carregar a tabela DADOS_INMET_HORARIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/pandas_tools.py:407: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  [pandas.api.types.is_datetime64tz_dtype(df[c]) for c in df.columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga conclu√≠da. Linhas carregadas: 8760\n",
      "   ‚úÖ Arquivos enviados: 1 chunk(s).\n",
      "\n",
      "üì• Baixando raw/2022/INMET_NE_PE_A322_GARANHUNS_01-01-2022_A_31-12-2022.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A322_GARANHUNS_01-01-2022_A_31-12-2022.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/garanhuns_2022_tratado.csv\n",
      "\n",
      "‚ùÑÔ∏è Conectando ao Snowflake para carregar a tabela DADOS_INMET_HORARIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/pandas_tools.py:407: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  [pandas.api.types.is_datetime64tz_dtype(df[c]) for c in df.columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga conclu√≠da. Linhas carregadas: 8760\n",
      "   ‚úÖ Arquivos enviados: 1 chunk(s).\n",
      "\n",
      "üì• Baixando raw/2023/INMET_NE_PE_A307_PETROLINA_01-01-2023_A_31-12-2023.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A307_PETROLINA_01-01-2023_A_31-12-2023.CSV\n",
      "   ‚ö† Removendo colunas 100% NaN: ['radiacao']\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/petrolina_2023_tratado.csv\n",
      "\n",
      "‚ùÑÔ∏è Conectando ao Snowflake para carregar a tabela DADOS_INMET_HORARIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/pandas_tools.py:407: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  [pandas.api.types.is_datetime64tz_dtype(df[c]) for c in df.columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga conclu√≠da. Linhas carregadas: 8760\n",
      "   ‚úÖ Arquivos enviados: 1 chunk(s).\n",
      "\n",
      "üì• Baixando raw/2023/INMET_NE_PE_A322_GARANHUNS_01-01-2023_A_31-12-2023.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A322_GARANHUNS_01-01-2023_A_31-12-2023.CSV\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8760\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/garanhuns_2023_tratado.csv\n",
      "\n",
      "‚ùÑÔ∏è Conectando ao Snowflake para carregar a tabela DADOS_INMET_HORARIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/pandas_tools.py:407: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  [pandas.api.types.is_datetime64tz_dtype(df[c]) for c in df.columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga conclu√≠da. Linhas carregadas: 8760\n",
      "   ‚úÖ Arquivos enviados: 1 chunk(s).\n",
      "\n",
      "üì• Baixando raw/2024/INMET_NE_PE_A307_PETROLINA_01-01-2024_A_31-12-2024.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A307_PETROLINA_01-01-2024_A_31-12-2024.CSV\n",
      "   ‚ö† Removendo colunas 100% NaN: ['radiacao']\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8784\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/petrolina_2024_tratado.csv\n",
      "\n",
      "‚ùÑÔ∏è Conectando ao Snowflake para carregar a tabela DADOS_INMET_HORARIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/pandas_tools.py:407: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  [pandas.api.types.is_datetime64tz_dtype(df[c]) for c in df.columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga conclu√≠da. Linhas carregadas: 8784\n",
      "   ‚úÖ Arquivos enviados: 1 chunk(s).\n",
      "\n",
      "üì• Baixando raw/2024/INMET_NE_PE_A322_GARANHUNS_01-01-2024_A_31-12-2024.CSV...\n",
      "\n",
      "üìÑ Lendo arquivo: INMET_NE_PE_A322_GARANHUNS_01-01-2024_A_31-12-2024.CSV\n",
      "   ‚ö† Removendo colunas 100% NaN: ['precipitacao']\n",
      "   ‚úÖ Registros ap√≥s tratamento: 8784\n",
      "   üíæ Salvo em: /home/jovyan/data/processed/garanhuns_2024_tratado.csv\n",
      "\n",
      "‚ùÑÔ∏è Conectando ao Snowflake para carregar a tabela DADOS_INMET_HORARIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/snowflake/connector/pandas_tools.py:407: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  [pandas.api.types.is_datetime64tz_dtype(df[c]) for c in df.columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Carga conclu√≠da. Linhas carregadas: 8784\n",
      "   ‚úÖ Arquivos enviados: 1 chunk(s).\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# LOOP PARA PROCESSAR, SALVAR CSV LOCALMENTE E CARREGAR NO POSTGRESQL\n",
    "# ==============================================================================\n",
    "\n",
    "for ano in anos:\n",
    "    pasta_ano = BASE_RAW / ano\n",
    "    \n",
    "    for cidade in cidades:\n",
    "        estacao_id = cidade.lower()\n",
    "        \n",
    "        arquivos = list(pasta_ano.glob(f\"{cidade}.CSV\"))\n",
    "        \n",
    "        if not arquivos:\n",
    "            print(f\"\\nüö´ Nenhum arquivo encontrado para {cidade} em {ano}\")\n",
    "            continue\n",
    "        \n",
    "        for arq in arquivos:\n",
    "            # 1. Processamento e Limpeza\n",
    "            df_tratado = processar_inmet(arq)\n",
    "            \n",
    "            # 2. Ajustes finais\n",
    "            df_tratado['estacao_id'] = estacao_id\n",
    "            \n",
    "            # 3. Resetar o √≠ndice para que datetime vire coluna\n",
    "            df_final = df_tratado.reset_index()\n",
    "            \n",
    "            # 4. Salva CSV Localmente (opcional)\n",
    "            nome_saida = f\"{estacao_id}_{ano}_tratado.csv\"\n",
    "            caminho_saida = BASE_PROC / nome_saida\n",
    "            df_tratado.to_csv(caminho_saida)\n",
    "            print(f\"   üíæ Salvo em: {caminho_saida}\")\n",
    "            \n",
    "            # 5. Carga para o PostgreSQL\n",
    "            carregar_para_postgres(df_final, 'dados_inmet_horario')\n",
    "\n",
    "print(\"\\nüéâ Processamento completo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f1c6a-7bf9-4ca1-8de3-d88b5a56892e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
